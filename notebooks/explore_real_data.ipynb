{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ssl\n",
    "import os\n",
    "import yaml\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from local_glm_boost import LocalGLMBooster\n",
    "from local_glm_boost.utils.tuning import tune_n_estimators\n",
    "from local_glm_boost.utils.logger import LocalGLMBoostLogger\n",
    "\n",
    "config_name = \"real_data_study\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Set up output folder, configuration file, run_id and logger\n",
    "script_dir = \"/home/heza7322/PycharmProjects/local-glm-boost/notebooks\"\n",
    "folder_path = os.path.join(script_dir, \"../data/results/\")\n",
    "config_path = os.path.join(script_dir, f\"{config_name}.yaml\")\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "if os.path.exists(folder_path) and os.listdir(folder_path):\n",
    "    run_id = (\n",
    "        max(\n",
    "            [int(folder_name.split(\"_\")[1]) for folder_name in os.listdir(folder_path)]\n",
    "            + [0]\n",
    "        )\n",
    "        + 1\n",
    "    )\n",
    "else:\n",
    "    run_id = 0\n",
    "\n",
    "output_path = os.path.join(folder_path, f\"run_{run_id}\")\n",
    "os.makedirs(output_path)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-04 11:48][run_13][Loading configuration]\n",
      "[2023-09-04 11:48][run_13][Loading data]\n"
     ]
    }
   ],
   "source": [
    "# Put the config in the output folder\n",
    "shutil.copyfile(config_path, f\"{output_path}/config.yaml\")\n",
    "\n",
    "# Set up logger\n",
    "logger = LocalGLMBoostLogger(\n",
    "    verbose=2,\n",
    "    output_path=output_path,\n",
    ")\n",
    "logger.append_format_level(f\"run_{run_id}\")\n",
    "\n",
    "# Load stuff from the config\n",
    "logger.log(\"Loading configuration\")\n",
    "n = config[\"n\"]\n",
    "features_to_use = config[\"features_to_use\"]\n",
    "target = config[\"target\"]\n",
    "weights = config[\"weights\"]\n",
    "distribution = config[\"distribution\"]\n",
    "n_estimators_max = config[\"n_estimators_max\"]\n",
    "learning_rate = config[\"learning_rate\"]\n",
    "min_samples_split = config[\"min_samples_split\"]\n",
    "min_samples_leaf = config[\"min_samples_leaf\"]\n",
    "max_depth = config[\"max_depth\"]\n",
    "glm_init = config[\"glm_init\"]\n",
    "random_seed = config[\"random_seed\"]\n",
    "n_splits = config[\"n_splits\"]\n",
    "test_size = config[\"test_size\"]\n",
    "parallel = config[\"parallel\"]\n",
    "stratified = config[\"stratified\"]\n",
    "n_jobs = config[\"n_jobs\"]\n",
    "\n",
    "# Load and preprocess data\n",
    "logger.log(\"Loading data\")\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "df_num = fetch_openml(data_id=41214, as_frame=True).data\n",
    "df_sev = fetch_openml(data_id=41215, as_frame=True).data\n",
    "df_sev_tot = df_sev.groupby('IDpol')['ClaimAmount'].sum()\n",
    "df = df_num.merge(df_sev_tot, left_on='IDpol', right_index=True, how='left')\n",
    "df.loc[df['ClaimAmount'].isna(), 'ClaimNb'] = 0\n",
    "df.loc[df['ClaimAmount'].isna(), 'ClaimAmount'] = 0\n",
    "df = df.loc[df['ClaimNb'] <=5]\n",
    "\n",
    "df['Exposure'] = df['Exposure'].clip(0, 1)\n",
    "df['Area'] = df['Area'].apply(lambda x: ord(x) - 65)\n",
    "df['VehGas'] = df['VehGas'].apply(lambda x: 1 if x == 'Regular' else 0)\n",
    "\n",
    "features = ['VehPower','VehAge','DrivAge','BonusMalus','Density','Area','VehGas']\n",
    "parallel_fit = []\n",
    "for feature in ['VehBrand','Region']:\n",
    "    dummies = pd.get_dummies(df[feature], prefix=feature)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    features += dummies.columns.tolist()\n",
    "    parallel_fit.append(dummies.columns.tolist())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
